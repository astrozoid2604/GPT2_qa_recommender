{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a66cc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "# 1. Settle data sourcing + Merge all 3 job datasets\n",
      "##########################################################################################\n",
      "Current Time: 04:52:09\n",
      "Dataset directory /Users/jameslim/Downloads/dataset is found.\n",
      "Zip filepath /Users/jameslim/Downloads/dataset/1-3m-linkedin-jobs-and-skills-2024.zip is found.\n",
      "All 3 dataset CSVs are found.\n",
      "All 3 dataset PARQUETs are found.\n",
      "Finished loading /Users/jameslim/Downloads/dataset/merged.parquet\n",
      "\n",
      "##########################################################################################\n",
      "# 2. Consolidate all columns into 1 column + Add linking words to form a sentence\n",
      "##########################################################################################\n",
      "Current Time: 04:52:09\n",
      "\n",
      "##########################################################################################\n",
      "# 3. Load pre-trained GPT2 tokenizer and model\n",
      "##########################################################################################\n",
      "Current Time: 04:52:09\n",
      "Finished loading pre-trained GPT2 model\n",
      "\n",
      "##########################################################################################\n",
      "# 4. Tokenize and format the merged dataset\n",
      "##########################################################################################\n",
      "Current Time: 04:52:11\n",
      "/Users/jameslim/Downloads/dataset/tokenized_text.pt is found.\n",
      "Finished reading /Users/jameslim/Downloads/dataset/tokenized_text.pt\n",
      "\n",
      "##########################################################################################\n",
      "# 5. Set up fine-tuning arguments + Define data collator for language modeling\n",
      "##########################################################################################\n",
      "Current Time: 04:54:09\n",
      "Finished setting up fine-tuning arguments.\n",
      "\n",
      "##########################################################################################\n",
      "# 6. Set up TensorBoard callback\n",
      "##########################################################################################\n",
      "Current Time: 04:54:09\n",
      "Finished setting up TensorBoard callback.\n",
      "\n",
      "##########################################################################################\n",
      "# 7. Set up Accelerator and Trainer \n",
      "##########################################################################################\n",
      "Current Time: 04:54:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslim/miniconda3/envs/gpd/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='6471340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     95/6471340 00:59 < 1143:41:07, 1.57 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset, TrainerCallback, is_tensorboard_available, EvalPrediction\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "\n",
    "cuda_availability= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if cuda_availability==\"cuda\":\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]       = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]    = \"0\" \n",
    "    dataset_dir                           = \"./dataset\"\n",
    "    #num_train_epochs                      =10 #KIV_SETTING1\n",
    "    num_train_epochs                      =5 #KIV_SETTING2\n",
    "    per_device_train_batch_size           =16\n",
    "    gradient_accumulation_steps           =1\n",
    "    save_steps                            =1000\n",
    "    save_total_limit                      =2\n",
    "    fp16                                  =True\n",
    "else:\n",
    "    dataset_dir                           = \"/Users/jameslim/Downloads/dataset\"\n",
    "    num_train_epochs                      =10\n",
    "    per_device_train_batch_size           =2\n",
    "    gradient_accumulation_steps           =1\n",
    "    save_steps                            =1000\n",
    "    save_total_limit                      =2\n",
    "    fp16                                  =False\n",
    "    \n",
    "\n",
    "kaggle_data_url  = \"asaniczka/1-3m-linkedin-jobs-and-skills-2024\"\n",
    "zip_path         = dataset_dir + \"/1-3m-linkedin-jobs-and-skills-2024.zip\"\n",
    "skills_csv_path  = dataset_dir + \"/job_skills.csv\"\n",
    "summary_csv_path = dataset_dir + \"/job_summary.csv\"\n",
    "posting_csv_path = dataset_dir + \"/linkedin_job_postings.csv\"\n",
    "skills_pq_path   = dataset_dir + \"/job_skills.parquet\"\n",
    "summary_pq_path  = dataset_dir + \"/job_summary.parquet\"\n",
    "posting_pq_path  = dataset_dir + \"/linkedin_job_postings.parquet\"\n",
    "merged_pq_path   = dataset_dir + \"/merged.parquet\"\n",
    "\n",
    "token_pt_path    = dataset_dir + \"/tokenized_text.pt\"\n",
    "#tensorboard_dir  = dataset_dir + \"/GPT2_tensorboard\" #KIV_SETTING1\n",
    "#train_dir        = dataset_dir + \"/GPT2_training\" #KIV_SETTING1\n",
    "#model_dir        = dataset_dir + \"/GPT2_finetuned_model\" #KIV_SETTING1\n",
    "tensorboard_dir  = dataset_dir + \"/GPT2_tensorboard_setting2\" #KIV_SETTING2\n",
    "train_dir        = dataset_dir + \"/GPT2_training_setting2\" #KIV_SETTING2\n",
    "model_dir        = dataset_dir + \"/GPT2_finetuned_model_setting2\" #KIV_SETTING1\n",
    "\n",
    "\n",
    "\n",
    "def print_time(x):\n",
    "    title_dict = {1:\"Settle data sourcing + Merge all 3 job datasets\",\n",
    "                  2:\"Consolidate all columns into 1 column + Add linking words to form a sentence\",\n",
    "                  3:\"Load pre-trained GPT2 tokenizer and model\",\n",
    "                  4:\"Tokenize and format the merged dataset\",\n",
    "                  5:\"Set up fine-tuning arguments + Define data collator for language modeling\",\n",
    "                  6:\"Set up TensorBoard callback\",\n",
    "                  7:\"Set up Accelerator and Trainer \",\n",
    "                  8:\"Save fine-tuned model\",\n",
    "                 }\n",
    "    print(\"\\n##########################################################################################\")\n",
    "    print(f\"# {x}. {title_dict[x]}\")\n",
    "    print(\"##########################################################################################\")\n",
    "    print(\"Current Time:\", time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n",
    "\n",
    "    \n",
    "##########################################################################################\n",
    "# 01. Settle data sourcing + Merge all 3 job datasets\n",
    "##########################################################################################\n",
    "print_time(1)\n",
    "file_cond = os.path.exists(dataset_dir)\n",
    "if file_cond==False: \n",
    "    print(f\"{dataset_dir} does not exist. Initiating makedirs...\")\n",
    "    os.makedirs(dataset_dir)\n",
    "else: print(f\"Dataset directory {dataset_dir} is found.\")\n",
    "\n",
    "file_cond = os.path.exists(zip_path)\n",
    "if file_cond==False:\n",
    "    print(f\"{zip_path} does not exist. Initiating download from KaggleAPI...\")\n",
    "    api = KaggleApi()\n",
    "    api.authenticate() #username=\"astrozoid2604\", key=\"271d95006019eb47f70fb5a5fe23e5a2\")\n",
    "    api.dataset_download_files(kaggle_data_url, path=dataset_dir, unzip=True)\n",
    "    print(f\"Finished downloading dataset {kaggle_data_url} from KaggleAPI\")\n",
    "else: print(f\"Zip filepath {zip_path} is found.\")\n",
    "        \n",
    "file_cond = os.path.exists(skills_csv_path) and os.path.exists(summary_csv_path) and os.path.exists(posting_csv_path)\n",
    "if file_cond==False:\n",
    "    print(f\"At least 1 dataset CSV file does not exist. Initiating extraction from downloaded zip file...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "    print(f\"Finished extracting files from {zip_path}\")\n",
    "else: print(\"All 3 dataset CSVs are found.\")\n",
    "\n",
    "file_cond = os.path.exists(skills_pq_path) and os.path.exists(summary_pq_path) or os.path.exists(posting_pq_path)\n",
    "if file_cond==False:\n",
    "    print(f\"At least 1 dataset PARQUET file does not exist. Initiating CSV-PARQUET conversion...\")\n",
    "    pd.read_csv(skills_csv_path).to_parquet(skills_pq_path)\n",
    "    pd.read_csv(summary_csv_path).to_parquet(summary_pq_path)\n",
    "    pd.read_csv(posting_csv_path).to_parquet(posting_pq_path)\n",
    "    print(f\"Finished converting CSV files to PARQUET files\")\n",
    "else: print(\"All 3 dataset PARQUETs are found.\")\n",
    "\n",
    "file_cond = os.path.exists(merged_pq_path)\n",
    "if file_cond==False:\n",
    "    print(f\"Merged dataset PARQUET {merged_pq_path} does not exist. Initiating dataset merging...\")\n",
    "    skill_df   = pd.read_parquet(skills_pq_path)\n",
    "    summary_df = pd.read_parquet(summary_pq_path)\n",
    "    posting_df = pd.read_parquet(posting_pq_path)\n",
    "    merged_df  = pd.merge(skill_df, summary_df, on=\"job_link\", how=\"inner\")\n",
    "    merged_df  = pd.merge(merged_df, posting_df[['job_link', 'job_title', 'company', 'job_location', 'search_country', 'job_level', 'job_type']], on=\"job_link\", how=\"inner\")\n",
    "    merged_df  = merged_df.dropna()\n",
    "    merged_df.to_parquet(merged_pq_path)\n",
    "    print(f\"Finished saving {merged_pq_path}\")\n",
    "    print(\"Shape of merged_df: \", merged_df.shape)\n",
    "elif file_cond==True and os.path.exists(token_pt_path)==False:\n",
    "    print(f\"Merged dataset PARQUET {merged_pq_path} is found. Initiating reading merged dataset...\")\n",
    "    merged_df  = pd.read_parquet(merged_pq_path).dropna()\n",
    "    print(f\"Finished loading {merged_pq_path}\")\n",
    "    print(\"Shape of merged_df: \", merged_df.shape)\n",
    "else: print(f\"Finished loading {merged_pq_path}\")\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# 02. Consolidate all columns into 1 column + Add linking words to form a sentence\n",
    "##########################################################################################\n",
    "print_time(2)\n",
    "def combine_cols(job_title, job_skills, job_level, job_type, job_summary, company, job_location, search_country):\n",
    "    string = \"The job title is \" + job_title + \". \"\n",
    "    string += \"The required skills are \" + job_skills + \". \"\n",
    "    string += \"The job level is \" + job_type + \". \"\n",
    "    string += \"The job type is \" + job_type + \". \"\n",
    "    string += \"Here is the job summary. \" + \" \".join(job_summary.split(\"\\n\")) + \". \"\n",
    "    string += \"The hiring company is \" + company + \". \"\n",
    "    string += \"The job is located at \" + job_location + \" in country \" + search_country + \". \"\n",
    "    string += \"This job posting comes from CS5344GROUP08LINKEDIN dataset.\"\n",
    "    return string\n",
    "\n",
    "file_cond = os.path.exists(token_pt_path)\n",
    "if file_cond==False:\n",
    "    combined_df = pd.DataFrame()\n",
    "    combined_df['Combined'] = merged_df.apply(lambda x: combine_cols(x['job_title'], x['job_skills'], x['job_level'], x['job_type'], x['job_summary'], x['company'], x['job_location'], x['search_country']), axis=1)\n",
    "    print(f\"Finished combining all columns of merged dataset.\")\n",
    "\n",
    "##########################################################################################\n",
    "# 03. Load pre-trained GPT2 tokenizer and model\n",
    "##########################################################################################\n",
    "print_time(3)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.to(torch.device(cuda_availability))\n",
    "print(f\"Finished loading pre-trained GPT2 model\")\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# 04. Tokenize and format the merged dataset\n",
    "##########################################################################################\n",
    "print_time(4)\n",
    "def tokenizer_with_progress(text_list):\n",
    "    tokenized_text = []\n",
    "    for text in tqdm(text_list, desc=\"GPT2 Tokenizer Progress Bar...\", ascii=False, ncols=75):\n",
    "        tokenized_text += [tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(cuda_availability)]\n",
    "    return tokenized_text\n",
    "\n",
    "file_cond = os.path.exists(token_pt_path)\n",
    "if file_cond==False:\n",
    "    print(f\"Token PARQUET file {token_pt_path} does not exist. Initiating text tokenization from merged dataset...\")\n",
    "    text_list = combined_df['Combined'].tolist()\n",
    "    tokenized_text = tokenizer_with_progress(text_list)\n",
    "    torch.save(tokenized_text, token_pt_path)\n",
    "    tokenized_text = [text.to(cuda_availability) for text in tokenized_text]\n",
    "    print(f\"Finished tokenization.\")\n",
    "else:\n",
    "    print(f\"{token_pt_path} is found.\")\n",
    "    tokenized_text = torch.load(token_pt_path) \n",
    "    tokenized_text = [text.to(cuda_availability) for text in tokenized_text]\n",
    "    print(f\"Finished reading {token_pt_path}\")\n",
    "\n",
    "##########################################################################################\n",
    "# 05. Set up fine-tuning arguments + Define data collator for language modeling\n",
    "##########################################################################################\n",
    "print_time(5)\n",
    "# training_args = TrainingArguments(output_dir                 =train_dir,\n",
    "#                                   overwrite_output_dir       =True,\n",
    "#                                   num_train_epochs           =num_train_epochs,\n",
    "#                                   per_device_train_batch_size=per_device_train_batch_size,\n",
    "#                                   gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#                                   save_steps                 =save_steps,\n",
    "#                                   save_total_limit           =save_total_limit,\n",
    "#                                   prediction_loss_only       =True,\n",
    "#                                   weight_decay               =0.01,\n",
    "#                                   save_strategy              =\"steps\",\n",
    "#                                   #evaluation_strategy        =\"no\", #KIV_SETTING1\n",
    "#                                   evaluation_strategy        =\"steps\", #KIV_SETTING2\n",
    "#                                   #do_eval                    =False,#KIV_SETTING1\n",
    "#                                   eval_steps                 = 40, #KIV_SETTING2\n",
    "#                                   logging_steps              = 40, #KIV_SETTING2\n",
    "#                                   load_best_model_at_end     = True, #KIV_SETTING2\n",
    "#                                   optim                      =\"adamw_torch\",\n",
    "#                                   resume_from_checkpoint     =train_dir,\n",
    "#                                   #learning_rate              =5e-5, #KIV_SETTING1\n",
    "#                                   learning_rate              =2e-4, #KIV_SETTING2\n",
    "#                                   logging_strategy           =\"steps\", \n",
    "#                                   seed                       =42,\n",
    "#                                   fp16                       =fp16,\n",
    "#                                  )\n",
    "training_args = TrainingArguments(output_dir                 =train_dir,\n",
    "                                  overwrite_output_dir       =True,\n",
    "                                  num_train_epochs           =5,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  gradient_accumulation_steps=1,\n",
    "                                  save_steps                 =500,\n",
    "                                  save_total_limit           =2,\n",
    "                                  prediction_loss_only       =True,\n",
    "                                  weight_decay               =0.04,\n",
    "                                  save_strategy              =\"epoch\",\n",
    "                                  evaluation_strategy        =\"epoch\", #KIV_SETTING2\n",
    "                                  load_best_model_at_end     = True, #KIV_SETTING2\n",
    "                                  optim                      =\"adamw_torch\",\n",
    "                                  resume_from_checkpoint     =train_dir,\n",
    "                                  learning_rate              =5e-5, #KIV_SETTING2\n",
    "                                  logging_strategy           =\"steps\",\n",
    "                                  logging_steps              =40,\n",
    "                                  seed                       =42,\n",
    "                                  fp16                       =True,\n",
    "                                 )\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(f\"Finished setting up fine-tuning arguments.\")\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# 06. Set up TensorBoard callback\n",
    "##########################################################################################\n",
    "print_time(6)\n",
    "def training_callback_fn(eval_loss, **kwargs):\n",
    "    \"\"\"\n",
    "    Callback function to write training loss into TensorBoard.\n",
    "    \"\"\"\n",
    "    global global_step\n",
    "    global_step += 1\n",
    "    writer.add_scalar(\"training_loss\", eval_loss, global_step=global_step)\n",
    "    \n",
    "def custom_rewrite_logs(d, mode):\n",
    "    new_d = {}\n",
    "    eval_prefix = \"eval_\"\n",
    "    eval_prefix_len = len(eval_prefix)\n",
    "    test_prefix = \"test_\"\n",
    "    test_prefix_len = len(test_prefix)\n",
    "    for k, v in d.items():\n",
    "        if mode == 'eval' and k.startswith(eval_prefix):\n",
    "            if k[eval_prefix_len:] == 'loss':\n",
    "                new_d[\"combined/\" + k[eval_prefix_len:]] = v\n",
    "        elif mode == 'test' and k.startswith(test_prefix):\n",
    "            if k[test_prefix_len:] == 'loss':\n",
    "                new_d[\"combined/\" + k[test_prefix_len:]] = v\n",
    "        elif mode == 'train':\n",
    "            if k == 'loss':\n",
    "                new_d[\"combined/\" + k] = v\n",
    "    return new_d\n",
    "\n",
    "class CombinedTensorBoardCallback(TrainerCallback):\n",
    "    def __init__(self, tb_writers=None):\n",
    "        has_tensorboard = is_tensorboard_available()\n",
    "        if not has_tensorboard: raise RuntimeError(\"TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.\")\n",
    "        if has_tensorboard:\n",
    "            try:\n",
    "                from torch.utils.tensorboard import SummaryWriter  # noqa: F401\n",
    "                self._SummaryWriter = SummaryWriter\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    from tensorboardX import SummaryWriter\n",
    "                    self._SummaryWriter = SummaryWriter\n",
    "                except ImportError:\n",
    "                    self._SummaryWriter = None\n",
    "        else:self._SummaryWriter = None\n",
    "        self.tb_writers = tb_writers\n",
    "    def _init_summary_writer(self, args, log_dir=None):\n",
    "        log_dir = log_dir or args.logging_dir\n",
    "        if self._SummaryWriter is not None:\n",
    "            self.tb_writers = dict(train=self._SummaryWriter(log_dir=os.path.join(log_dir, 'train')),\n",
    "                                   eval=self._SummaryWriter(log_dir=os.path.join(log_dir, 'eval')))\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        if not state.is_world_process_zero: return\n",
    "        log_dir = None\n",
    "        if state.is_hyper_param_search:\n",
    "            trial_name = state.trial_name\n",
    "            if trial_name is not None:\n",
    "                log_dir = os.path.join(args.logging_dir, trial_name)\n",
    "\n",
    "        if self.tb_writers is None:\n",
    "            self._init_summary_writer(args, log_dir)\n",
    "\n",
    "        for k, tbw in self.tb_writers.items():\n",
    "            tbw.add_text(\"args\", args.to_json_string())\n",
    "            if \"model\" in kwargs:\n",
    "                model = kwargs[\"model\"]\n",
    "                if hasattr(model, \"config\") and model.config is not None:\n",
    "                    model_config_json = model.config.to_json_string()\n",
    "                    tbw.add_text(\"model_config\", model_config_json)\n",
    "            # Version of TensorBoard coming from tensorboardX does not have this method.\n",
    "            if hasattr(tbw, \"add_hparams\"):\n",
    "                tbw.add_hparams(args.to_sanitized_dict(), metric_dict={})\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not state.is_world_process_zero:\n",
    "            return\n",
    "\n",
    "        if self.tb_writers is None:\n",
    "            self._init_summary_writer(args)\n",
    "\n",
    "        for tbk, tbw in self.tb_writers.items():\n",
    "            logs_new = custom_rewrite_logs(logs, mode=tbk)\n",
    "            for k, v in logs_new.items():\n",
    "                if isinstance(v, (int, float)):\n",
    "                    tbw.add_scalar(k, v, state.global_step)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Trainer is attempting to log a value of \"\n",
    "                        f'\"{v}\" of type {type(v)} for key \"{k}\" as a scalar. '\n",
    "                        \"This invocation of Tensorboard's writer.add_scalar() \"\n",
    "                        \"is incorrect so we dropped this attribute.\"\n",
    "                    )\n",
    "            tbw.flush()\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        for tbw in self.tb_writers.values():\n",
    "            tbw.close()\n",
    "        self.tb_writers = None\n",
    "    \n",
    "writer = SummaryWriter(log_dir=tensorboard_dir)\n",
    "global_step = 0    # Initialize global step\n",
    "print(f\"Finished setting up TensorBoard callback.\")\n",
    "\n",
    "##########################################################################################\n",
    "# 07. Set up Accelerator and Trainer \n",
    "##########################################################################################\n",
    "print_time(7)\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, training_args, data_collator = accelerator.prepare(model, training_args, data_collator)\n",
    "\n",
    "# training_size = int(0.8*len(tokenized_text))\n",
    "training_size = int(0.85*len(tokenized_text))\n",
    "ds_train, ds_valid = tokenized_text[:training_size:], tokenized_text[training_size:] #KIV_SETTING2\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    #train_dataset=tokenized_text, #KIV_SETTING1\n",
    "    train_dataset=ds_train, #KIV_SETTING2\n",
    "    #eval_dataset=None,  # Pass None for eval_dataset #KIV_SETTING1\n",
    "    eval_dataset=ds_valid,  #KIV_SETTING2\n",
    "    compute_metrics=compute_metrics, #KIV_SETTING2\n",
    "    #callbacks=[training_callback_fn],  # Add the callback function\n",
    "    callbacks=[CombinedTensorBoardCallback]\n",
    ")\n",
    "trainer.train()\n",
    "print(\"Finished fine-tuning GPT2 model.\")\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# 08. Save fine-tuned model\n",
    "##########################################################################################\n",
    "print_time(8)\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    trainer.save_model(model_dir)\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)  # Don't forget to save the tokenizer as well\n",
    "    print(f\"Finished saving fine-tuned model at {model_dir}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (gpd)",
   "language": "python",
   "name": "gpd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
